{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e67c681-a2b2-4da1-a61e-e80ab967a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import tensor_parallel as tp\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from scipy.stats import entropy\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM\n",
    "from  transformers.generation.logits_process import LogitsProcessorList\n",
    "from  transformers.generation.stopping_criteria import StoppingCriteriaList, MaxLengthCriteria\n",
    "import transformers\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "TASKS = [\n",
    "        'abstract_algebra',\n",
    "        'anatomy',\n",
    "        'astronomy',\n",
    "        'business_ethics',\n",
    "        'clinical_knowledge',\n",
    "        'college_biology',\n",
    "        'college_chemistry',\n",
    "        'college_computer_science',\n",
    "        'college_mathematics',\n",
    "        'college_medicine',\n",
    "        'college_physics',\n",
    "        'computer_security',\n",
    "        'conceptual_physics',\n",
    "        'econometrics',\n",
    "        'electrical_engineering',\n",
    "        'elementary_mathematics',\n",
    "        'formal_logic',\n",
    "        'global_facts',\n",
    "        'high_school_biology',\n",
    "        'high_school_chemistry',\n",
    "        'high_school_computer_science',\n",
    "        'high_school_european_history',\n",
    "        'high_school_geography',\n",
    "        'high_school_government_and_politics',\n",
    "        'high_school_macroeconomics',\n",
    "        'high_school_mathematics',\n",
    "        'high_school_microeconomics',\n",
    "        'high_school_physics',\n",
    "        'high_school_psychology',\n",
    "        'high_school_statistics',\n",
    "        'high_school_us_history',\n",
    "        'high_school_world_history',\n",
    "        'human_aging',\n",
    "        'human_sexuality',\n",
    "        'international_law',\n",
    "        'jurisprudence',\n",
    "        'logical_fallacies',\n",
    "        'machine_learning',\n",
    "        'management',\n",
    "        'marketing',\n",
    "        'medical_genetics',\n",
    "        'miscellaneous',\n",
    "        'moral_disputes',\n",
    "        'moral_scenarios',\n",
    "        'nutrition',\n",
    "        'philosophy',\n",
    "        'prehistory',\n",
    "        'professional_accounting',\n",
    "        'professional_law',\n",
    "        'professional_medicine',\n",
    "        'professional_psychology',\n",
    "        'public_relations',\n",
    "        'security_studies', \n",
    "        'sociology',\n",
    "        'us_foreign_policy',\n",
    "        'virology',\n",
    "        'world_religions']\n",
    "\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def compute_metric(output_filename):\n",
    "    with open(output_filename, 'r') as f:\n",
    "        run_results = json.load(f)\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    for task in run_results:\n",
    "        acc = 0\n",
    "        pred_answers = run_results[task]['pred_answers']\n",
    "        gold_answers = run_results[task]['gold_answers']\n",
    "        for pred, gold in zip(pred_answers, gold_answers):\n",
    "            if pred == gold: acc += 1\n",
    "        print(\"ACC-%s: %.4f\" % (task, acc/len(gold_answers)))\n",
    "        total_acc += acc\n",
    "        total_num += len(gold_answers)\n",
    "    print(\"ACC-all: %.4f\" % (total_acc/total_num))\n",
    "\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "def format_example(df, idx, include_answer=True):\n",
    "    prompt = df.iloc[idx, 0]\n",
    "    k = df.shape[1] - 2\n",
    "    for j in range(k):\n",
    "        prompt += \"\\n{}. {}\".format(choices[j], df.iloc[idx, j+1])\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    if include_answer:\n",
    "        prompt += \" {}\\n\\n\".format(df.iloc[idx, k + 1])\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(train_df, subject, k=-1):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(format_subject(subject))\n",
    "    if k == -1:\n",
    "        k = train_df.shape[0]\n",
    "    for i in range(k):\n",
    "        prompt += format_example(train_df, i)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# def custom_stopping_criteria(input_ids, score, **kwargs):\n",
    "#     stop_ids = [29871, 13, 13] # \\n\\n \n",
    "#     return input_ids[-len(stop_ids)]\n",
    "\n",
    "def prepare_input(tokenizer, prompts):\n",
    "    input_tokens = tokenizer.batch_encode_plus(prompts, return_tensors=\"pt\", padding=True)\n",
    "    input_tokens = {k:input_tokens[k] for k in input_tokens if k in [\"input_ids\", \"attention_mask\"]}\n",
    "    for t in input_tokens:\n",
    "        if torch.is_tensor(input_tokens[t]):\n",
    "            input_tokens[t] = input_tokens[t].to('cuda')\n",
    "\n",
    "    return input_tokens\n",
    "\n",
    "def load(ckpt_dir, model_type):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "\n",
    "    if model_type == 'llama':\n",
    "        # we use tensor parallel for loading llama\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(ckpt_dir, use_fast=False, padding_side=\"left\")\n",
    "        \n",
    "        model = LlamaForCausalLM.from_pretrained(ckpt_dir, attention_dropout=.1, low_cpu_mem_usage = True, torch_dtype=torch.float16)\n",
    "        model = tp.tensor_parallel(model, [i for i in range(n_gpus)])\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'flan':\n",
    "        # we use tensor parallel for loading llama\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, use_fast=False, padding_side=\"left\")\n",
    "        \n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(ckpt_dir, low_cpu_mem_usage = True, torch_dtype=torch.float16)\n",
    "        model = tp.tensor_parallel(model, [i for i in range(n_gpus)])\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'falcon':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir, device_map = 'balanced_low_0', torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'moss':\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        config = AutoConfig.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "        model.tie_weights()\n",
    "        model = load_checkpoint_and_dispatch(model, model_path, device_map=\"auto\", no_split_module_classes=[\"MossBlock\"], dtype=torch.float16)\n",
    "        \n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'guanaco':\n",
    "\n",
    "\n",
    "\n",
    "        model_name = \"llama-65b\"\n",
    "        adapters_name = 'guanaco-65b'\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            load_in_4bit=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            max_memory= {i: '24000MB' for i in range(torch.cuda.device_count())},\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type='nf4'\n",
    "            ),\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, adapters_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    elif model_type == 'vicuna':\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir, device_map = 'balanced_low_0', revision=\"main\", trust_remote_code=False)\n",
    "\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "        \n",
    "    elif model_type == 'starcoder':\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir,device_map = 'balanced_low_0', trust_remote_code=True)\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "        \n",
    "\n",
    "    else:\n",
    "        # mpt-30b's tokenizer only has the fast version\n",
    "        use_fast = \"mosaicml/mpt-30b\" in ckpt_dir\n",
    "        # however, tensor parallel for running falcon will occur bugs\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, use_fast = use_fast, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir, device_map = 'balanced_low_0', torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            if tokenizer.eos_token_id is not None:\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            else:\n",
    "                tokenizer.pad_token_id = 0\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def batch_split(prompts, batch_num):\n",
    "    batch_prompts = []\n",
    "    mini_batch = []\n",
    "    for prompt in prompts:\n",
    "        mini_batch.append(prompt)\n",
    "        if len(mini_batch) == batch_num:\n",
    "            batch_prompts.append(mini_batch)\n",
    "            mini_batch = []\n",
    "    if len(mini_batch) != 0:\n",
    "        batch_prompts.append(mini_batch)\n",
    "    return batch_prompts\n",
    "\n",
    "\n",
    "def batch_infer(model, tokenizer, prompts):\n",
    "    batch_size = 8\n",
    "    answers = []\n",
    "    for batch_input in tqdm(batch_split(prompts, batch_size)):\n",
    "        encode_inputs = prepare_input(tokenizer, batch_input)\n",
    "        outputs = model.generate(**encode_inputs, max_new_tokens=1, pad_token_id=tokenizer.pad_token_id)\n",
    "        answers.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "    answers = [answer[-1] for answer in answers]\n",
    "    return answers\n",
    "\n",
    "def confidence_infer(model, tokenizer, prompts, token_confidence_funcs, confidence_aggregation_funcs, sequence_confidence_funcs):\n",
    "    answers = []\n",
    "    confidences = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        answer, confidence_dict = generate_with_confidence(model, tokenizer, prompt, 1, token_confidence_funcs, confidence_aggregation_funcs, sequence_confidence_funcs )\n",
    "        \n",
    "        answers.extend(answer)\n",
    "        confidences.extend(confidence_dict)\n",
    "        \n",
    "    return answers,confidences\n",
    "\n",
    "\n",
    "\n",
    "def min_confidence_agg(values, all_ids, model):\n",
    "    return 'min', min(values)\n",
    "def max_confidence_agg(values, all_ids, model):\n",
    "    return 'max', max(values)\n",
    "def avg_confidence_agg(values, all_ids, model):\n",
    "    return 'avg', torch.mean(torch.cat(tuple([v.unsqueeze(0) for v in values])),dim = -1)\n",
    "def attention_weighted_agg(values, all_ids, model):\n",
    "    #based on Guan et. al. 2023 Shifting Attention to Relevance\n",
    "    with torch.no_grad():\n",
    "        model_inputs = model.prepare_inputs_for_generation(all_ids, )\n",
    "        outputs = model(\n",
    "            **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=False,\n",
    "        )\n",
    "    attn_weights = outputs['attentions'][0][-1,-1,-1,-1*len(values):]\n",
    "    return 'attention_weighted', torch.dot(torch.cat(tuple([v.unsqueeze(0) for v in values])).half() ,attn_weights)/torch.sum(attn_weights)\n",
    "\n",
    "def logit_confidence(next_tokens_scores, next_token, all_ids, model):\n",
    "    return 'logit', torch.max(next_tokens_scores[-1,-1,:])\n",
    "    \n",
    "def softmax_confidence(next_tokens_scores, next_token, all_ids, model):\n",
    "    return 'softmax' , torch.max(torch.nn.functional.softmax(next_tokens_scores[-1,-1,:], dim = -1))\n",
    "    \n",
    "def entropy_confidence(next_tokens_scores, next_token, all_ids, model):\n",
    "    return 'entropy' , Categorical(probs=torch.nn.functional.softmax(next_tokens_scores[-1,-1,:], dim = -1)).entropy()\n",
    "\n",
    "def ensemble_entropy_confidence(next_tokens_scores, next_token, all_ids, model, n_ensemble=5):\n",
    "    model.train()\n",
    "    estimate_vector = torch.zeros(next_tokens_scores.shape[-1], dtype = torch.float32, device = model.device)\n",
    "    logits_processor = LogitsProcessorList()\n",
    "\n",
    "    for i in range(n_ensemble):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_inputs = model.prepare_inputs_for_generation(all_ids, )\n",
    "            outputs = model(\n",
    "                **model_inputs,\n",
    "                    return_dict=True,\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False,\n",
    "            )\n",
    "        next_tokens_scores = logits_processor(outputs['logits'][:,-1,:], outputs['logits'])\n",
    "        next_token = torch.argmax(next_tokens_scores[:,-1,:])\n",
    "        estimate_vector[next_token.item()] += 1\n",
    "        \n",
    "    estimate_vector = estimate_vector/n_ensemble # probabilities should sum to 1\n",
    "    model.eval()\n",
    "    return 'ensemble_entropy' , Categorical(probs=estimate_vector).entropy() \n",
    "\n",
    "def MMLU_self_reflection_confidence_promptv1(n_prompt_tokens,all_ids, model, tokenizer, prompt):\n",
    "    proposed_answer = tokenizer.decode(all_ids[-1,-1*(all_ids.shape[-1] - n_prompt_tokens):])\n",
    "    prompt = prompt[:-7] # remove the \"Answer:\" at the end of the prompt\n",
    "    prompt = prompt + f'''Proposed Answer: {proposed_answer}\n",
    "Is the proposed answer:\n",
    "(A) True\n",
    "(B) False\n",
    "The proposed answer is: '''\n",
    "    encode_inputs = prepare_prompt(tokenizer, prompt)\n",
    "    outputs = model.generate(**encode_inputs, max_new_tokens=1, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "    return 'self_reflection_promptv1',1 if outputs[-1,-1].item() == tokenizer.encode('A')[-1] else 0 #return 1 if model outputs 'A' else 0\n",
    "\n",
    "def prepare_prompt(tokenizer, prompt):\n",
    "    input_tokens = tokenizer.encode_plus(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_tokens = {\n",
    "            k:input_tokens[k] for k in input_tokens if k in [\"input_ids\", \"attention_mask\"]\n",
    "    }\n",
    "    for t in input_tokens:\n",
    "        if torch.is_tensor(input_tokens[t]):\n",
    "            input_tokens[t] = input_tokens[t].to('cuda')\n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_with_confidence(model, tokenizer, prompt, max_new_tokens, token_confidence_funcs, confidence_aggregation_funcs, sequence_confidence_funcs ):\n",
    "    all_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device) \n",
    "    n_prompt_tokens = all_ids.shape[-1]\n",
    "    logits_processor = LogitsProcessorList()\n",
    "    stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=n_prompt_tokens+max_new_tokens)])\n",
    "    pad_token_id = model.generation_config.pad_token_id\n",
    "    eos_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "    all_token_confidences = {}\n",
    "    sequence_confidences = {}\n",
    "\n",
    "    while True:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_inputs = model.prepare_inputs_for_generation(all_ids, )\n",
    "        \n",
    "\n",
    "            outputs = model(\n",
    "                **model_inputs,\n",
    "                    return_dict=True,\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False,\n",
    "            )\n",
    "        \n",
    "        next_tokens_scores = logits_processor(outputs['logits'][:,-1,:], outputs['logits'])\n",
    "        next_token = torch.argmax(next_tokens_scores[:,-1,:])\n",
    "\n",
    "        token_confidences = { id: val for id, val in [func(next_tokens_scores, next_token, all_ids, model) for func in token_confidence_funcs]}\n",
    "\n",
    "        for id, val in token_confidences.items():\n",
    "            all_token_confidences[id] = all_token_confidences.get(id,[])\n",
    "            all_token_confidences[id].append(val)\n",
    "        \n",
    "        all_ids = torch.cat([all_ids,next_token.unsqueeze(0).unsqueeze(0)],axis = -1)\n",
    "\n",
    "        if stopping_criteria(all_ids, next_tokens_scores):\n",
    "            break\n",
    "\n",
    "\n",
    "    for func in confidence_aggregation_funcs:\n",
    "        for token_confidence_id , values in all_token_confidences.items():\n",
    "            agg_id , val = func(values, all_ids, model)\n",
    "            sequence_confidences[agg_id+'|'+token_confidence_id] = val\n",
    "\n",
    "    for func in sequence_confidence_funcs:\n",
    "        seq_confidence_id , val = func(n_prompt_tokens,all_ids, model, tokenizer, prompt)\n",
    "        sequence_confidences[seq_confidence_id] = val       \n",
    "\n",
    "    \n",
    "    answer = tokenizer.decode(all_ids[-1,-1*max_new_tokens:])\n",
    "    return [answer], sequence_confidences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5ceba-af89-41f5-8505-a5055538baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = 'models/Llama-2-7b-hf/'\n",
    "model_type = 'llama'\n",
    "model, tokenizer = load(ckpt_dir, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41236eac-51cb-4a62-bccc-0f0bdd2509ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing abstract_algebra ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [01:58<00:40,  1.54s/it]"
     ]
    }
   ],
   "source": [
    "ckpt_dir = 'models/Llama-2-7b-hf/'\n",
    "param_size = '7'\n",
    "model_type = 'llama'\n",
    "data_dir = 'benchmarks/MMLU/'\n",
    "benchmark = 'MMLU'\n",
    "ntrain = 5\n",
    "\n",
    "run_results = {}\n",
    "output_filename = 'testrun_results_%s_%s_%sb.json' % (benchmark, model_type, param_size)\n",
    "\n",
    "confidence_aggregation_funcs = [min_confidence_agg,max_confidence_agg,avg_confidence_agg,attention_weighted_agg]\n",
    "token_confidence_funcs = [logit_confidence,softmax_confidence,entropy_confidence, ensemble_entropy_confidence]\n",
    "sequence_confidence_funcs = [MMLU_self_reflection_confidence_promptv1, ]\n",
    "\n",
    "\n",
    "#model, tokenizer = load(ckpt_dir, model_type)\n",
    "start_time = time.time()\n",
    "for task in TASKS[:1]:\n",
    "    print('Testing %s ...' % task)\n",
    "    records = []\n",
    "    dev_df = pd.read_csv(os.path.join(data_dir, \"dev\", task + \"_dev.csv\"), header=None)[:ntrain]\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, \"test\", task + \"_test.csv\"), header=None)\n",
    "    for i in range(test_df.shape[0]):\n",
    "        # get prompt and make sure it fits\n",
    "        k = ntrain\n",
    "        prompt_end = format_example(test_df, i, include_answer=False)\n",
    "        train_prompt = gen_prompt(dev_df, task, k)\n",
    "        prompt = train_prompt + prompt_end\n",
    "        while len(tokenizer.tokenize(prompt)) + 1> 2048: # bos token\n",
    "            prompt_split = prompt.split(\"\\n\\n\")\n",
    "            prompt_split.pop(1)\n",
    "            prompt = '\\n\\n'.join(prompt_split)\n",
    "        label = test_df.iloc[i, test_df.shape[1]-1]\n",
    "        records.append({'prompt':prompt, 'answer':label})\n",
    "\n",
    "    pred_answers, confidences = confidence_infer(model, tokenizer, [record['prompt'] for record in records],token_confidence_funcs, confidence_aggregation_funcs, sequence_confidence_funcs)\n",
    "    gold_answers = [record['answer'] for record in records]\n",
    "    run_results[task] = {'pred_answers':pred_answers, 'gold_answers':gold_answers, 'confidences' :confidences}\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(run_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "compute_metric(output_filename)\n",
    "end_time = time.time()\n",
    "print(\"total run time %.2f\" % (end_time - start_time))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e7cf15-a498-4edd-853c-72f2c27c04ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c97f7beb564504ab03020c1d346860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce23598-1779-4293-97dd-176991ac6476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad27f17c-b018-40b8-af97-6081d236dec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B'] {'min|logit': tensor(23.6562, device='cuda:0'), 'min|softmax': tensor(0.5079, device='cuda:0'), 'min|entropy': tensor(1.2472, device='cuda:0'), 'min|ensemble_entropy': tensor(1.1921e-07, device='cuda:0'), 'max|logit': tensor(23.6562, device='cuda:0'), 'max|softmax': tensor(0.5079, device='cuda:0'), 'max|entropy': tensor(1.2472, device='cuda:0'), 'max|ensemble_entropy': tensor(1.1921e-07, device='cuda:0'), 'avg|logit': tensor(23.6562, device='cuda:0'), 'avg|softmax': tensor(0.5079, device='cuda:0'), 'avg|entropy': tensor(1.2472, device='cuda:0'), 'avg|ensemble_entropy': tensor(1.1921e-07, device='cuda:0'), 'attention_weighted|logit': tensor(23.6406, device='cuda:0', dtype=torch.float16), 'attention_weighted|softmax': tensor(0.5078, device='cuda:0', dtype=torch.float16), 'attention_weighted|entropy': tensor(1.2471, device='cuda:0', dtype=torch.float16), 'attention_weighted|ensemble_entropy': tensor(0., device='cuda:0', dtype=torch.float16), 'self_reflection_promptv1': 0}\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 1\n",
    "all_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device) \n",
    "n_prompt_tokens = all_ids.shape[-1]\n",
    "logits_processor = LogitsProcessorList()\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=n_prompt_tokens+max_new_tokens)])\n",
    "pad_token_id = model.generation_config.pad_token_id\n",
    "eos_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "all_token_confidences = {}\n",
    "sequence_confidences = {}\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_inputs = model.prepare_inputs_for_generation(all_ids, )\n",
    "    \n",
    "\n",
    "        outputs = model(\n",
    "            **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "        )\n",
    "    \n",
    "    next_tokens_scores = logits_processor(outputs['logits'][:,-1,:], outputs['logits'])\n",
    "    next_token = torch.argmax(next_tokens_scores[:,-1,:])\n",
    "\n",
    "    token_confidences = { id: val for id, val in [func(next_tokens_scores, next_token, all_ids, model) for func in token_confidence_funcs]}\n",
    "\n",
    "    for id, val in token_confidences.items():\n",
    "        all_token_confidences[id] = all_token_confidences.get(id,[])\n",
    "        all_token_confidences[id].append(val)\n",
    "    \n",
    "    all_ids = torch.cat([all_ids,next_token.unsqueeze(0).unsqueeze(0)],axis = -1)\n",
    "\n",
    "    if stopping_criteria(all_ids, next_tokens_scores):\n",
    "        break\n",
    "\n",
    "\n",
    "for func in confidence_aggregation_funcs:\n",
    "    for token_confidence_id , values in all_token_confidences.items():\n",
    "        agg_id , val = func(values, all_ids, model)\n",
    "        sequence_confidences[agg_id+'|'+token_confidence_id] = val\n",
    "\n",
    "for func in sequence_confidence_funcs:\n",
    "    seq_confidence_id , val = func(n_prompt_tokens,all_ids, model, tokenizer, prompt)\n",
    "    sequence_confidences[seq_confidence_id] = val       \n",
    "\n",
    "\n",
    "answer = tokenizer.decode(all_ids[-1,-1*max_new_tokens:])\n",
    "print( [answer], sequence_confidences)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ec2d69c0-f6ce-4080-a8f4-21b1a02c34bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3750\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3748\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:1001\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    993\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    998\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1001\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[1;32m   1003\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1004\u001b[0m     }\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:976\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    974\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 976\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(all_ids.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1afae6d5-4cde-4385-8c3a-4ccc030cfa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "de105899-b186-4393-8689-95daacddd58f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m proposed_answer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_prompt_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3750\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3748\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:1001\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    993\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    998\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1001\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[1;32m   1003\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1004\u001b[0m     }\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:976\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    974\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 976\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "proposed_answer = tokenizer.decode(all_ids[-1*(all_ids.shape[-1] - n_prompt_tokens):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0bd2ae70-febe-413e-945e-86189b67e627",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#[:,-1*(all_ids.shape[-1] - n_prompt_tokens):])\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3750\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3748\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:1001\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    993\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    998\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1001\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[1;32m   1003\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1004\u001b[0m     }\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:976\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    974\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 976\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(all_ids[:,-1*(all_ids.shape[-1] - n_prompt_tokens):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5f41d629-4428-49f9-a5ec-c23b6b4db749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   450,  1494,   526,  2999,  7348,  5155,   313,  2541,  6089,\n",
       "         29897,  1048, 29871,  9846,  9623, 29889,    13,    13, 12542,   599,\n",
       "           274,   297,   796, 29918, 29941,  1316,   393,   796, 29918, 29941,\n",
       "         29961, 29916, 29962, 14571, 29916, 29985, 29906,   718,   274, 29897,\n",
       "           338,   263,  1746, 29889,    13, 29909, 29889, 29871, 29900,    13,\n",
       "         29933, 29889, 29871, 29896,    13, 29907, 29889, 29871, 29906,    13,\n",
       "         29928, 29889, 29871, 29941,    13, 22550, 29901,   350,    13,    13,\n",
       "         14473, 29871, 29896,   891,   960,   263, 29950,   338,   385,  1543,\n",
       "           310,   263,  7329,  2318, 29892,   769,   891, 29874, 29950, 29989,\n",
       "          1933,  2247,   891, 29874, 29989, 29889,  6666,   882, 29871, 29906,\n",
       "           891,   960,   379,   322,   476,   526,  1014, 13155,   310,   402,\n",
       "           769,   379, 29968,   338,   263, 24410,   310,   402, 29889,    13,\n",
       "         29909, 29889,  5852, 29892,  5852,    13, 29933, 29889,  7700, 29892,\n",
       "          7700,    13, 29907, 29889,  5852, 29892,  7700,    13, 29928, 29889,\n",
       "          7700, 29892,  5852,    13, 22550, 29901,   350,    13,    13, 14473,\n",
       "         29871, 29896,   891,  7569,  1543,   310,   263,  2318, 16785,   263,\n",
       "          5094, 28746, 24410,   310,   278,  2318, 29889,  6666,   882, 29871,\n",
       "         29906,   891,   450, 18348,  2318,   317, 29918, 29896, 29900,   756,\n",
       "         29871, 29896, 29900,  3161, 29889,    13, 29909, 29889,  5852, 29892,\n",
       "          5852,    13, 29933, 29889,  7700, 29892,  7700,    13, 29907, 29889,\n",
       "          5852, 29892,  7700,    13, 29928, 29889,  7700, 29892,  5852,    13,\n",
       "         22550, 29901,   315,    13,    13, 14473, 29871, 29896, 29989,  7569,\n",
       "           740,   515,   263,  8093,   731, 11480,  3528,  1818,   367,   697,\n",
       "           304,   697, 29889,  6666,   882, 29871, 29906,   891,  7569, 24410,\n",
       "           310,   385,   633, 27185,  2318,   338,   633, 27185, 29889,    13,\n",
       "         29909, 29889,  5852, 29892,  5852,    13, 29933, 29889,  7700, 29892,\n",
       "          7700,    13, 29907, 29889,  5852, 29892,  7700,    13, 29928, 29889,\n",
       "          7700, 29892,  5852,    13, 22550, 29901,   319,    13,    13, 12542,\n",
       "           278, 17443,   310,   278,  9228, 29871, 29906, 29999, 29889,    13,\n",
       "         29909, 29889, 29871, 29900,    13, 29933, 29889, 29871, 29941,    13,\n",
       "         29907, 29889, 29871, 29896, 29906,    13, 29928, 29889, 29871, 29941,\n",
       "         29900,    13, 22550, 29901,   319,    13,    13, 14473, 29871, 29896,\n",
       "           891,  7569, 10839,   297,   263,  9228,   338,   263,  1014,  5393,\n",
       "           310,   278,  9228, 29889,  6666,   882, 29871, 29906,   891,  7569,\n",
       "          1014,  5393,   310,  1432,  9228,   338,   385, 10839,   310,   278,\n",
       "          9228, 29889,    13, 29909, 29889,  5852, 29892,  5852,    13, 29933,\n",
       "         29889,  7700, 29892,  7700,    13, 29907, 29889,  5852, 29892,  7700,\n",
       "            13, 29928, 29889,  7700, 29892,  5852,    13, 22550, 29901,   319]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1f361af3-6ac4-4940-b829-2c27a313e51b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3750\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3748\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:1001\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    993\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    998\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1001\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[1;32m   1003\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1004\u001b[0m     }\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:976\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    974\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 976\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00998385-41e1-4b5a-add3-e302b7be5018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(93634.0234, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(next_tokens_scores[-1,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f62eaa6-f6c9-4fde-bcff-18bb69800568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(ckpt_dir, model_type):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "\n",
    "    if model_type == 'llama':\n",
    "        # we use tensor parallel for loading llama\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(ckpt_dir, use_fast=False, padding_side=\"left\")\n",
    "        \n",
    "        model = LlamaForCausalLM.from_pretrained(ckpt_dir, low_cpu_mem_usage = True, torch_dtype=torch.float16)\n",
    "        model = tp.tensor_parallel(model, [i for i in range(n_gpus)])\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'flan':\n",
    "        # we use tensor parallel for loading llama\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, use_fast=False, padding_side=\"left\")\n",
    "        \n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(ckpt_dir, low_cpu_mem_usage = True, torch_dtype=torch.float16)\n",
    "        model = tp.tensor_parallel(model, [i for i in range(n_gpus)])\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'falcon':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir, device_map = 'balanced_low_0', torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'moss':\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        config = AutoConfig.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "        model.tie_weights()\n",
    "        model = load_checkpoint_and_dispatch(model, model_path, device_map=\"auto\", no_split_module_classes=[\"MossBlock\"], dtype=torch.float16)\n",
    "        \n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'guanaco':\n",
    "\n",
    "\n",
    "\n",
    "        model_name = \"llama-65b\"\n",
    "        adapters_name = 'guanaco-65b'\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            load_in_4bit=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            max_memory= {i: '24000MB' for i in range(torch.cuda.device_count())},\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type='nf4'\n",
    "            ),\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, adapters_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    elif model_type == 'vicuna':\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir, device_map = 'balanced_low_0', revision=\"main\", trust_remote_code=False)\n",
    "\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "        \n",
    "    elif model_type == 'starcoder':\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir,device_map = 'balanced_low_0', trust_remote_code=True)\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "        \n",
    "\n",
    "    else:\n",
    "        # mpt-30b's tokenizer only has the fast version\n",
    "        use_fast = \"mosaicml/mpt-30b\" in ckpt_dir\n",
    "        # however, tensor parallel for running falcon will occur bugs\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, use_fast = use_fast, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir, device_map = 'balanced_low_0', torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            if tokenizer.eos_token_id is not None:\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            else:\n",
    "                tokenizer.pad_token_id = 0\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def token_level_confidence():\n",
    "    pass\n",
    "\n",
    "\n",
    "def infer_with_confidence(model, tokenizer, prompts, token_confidence_funcs, confidence_aggregation_funcs):\n",
    "    batch_size = 1\n",
    "    answers = []\n",
    "    for batch_input in tqdm(batch_split(prompts, batch_size)):\n",
    "        encode_inputs = prepare_input(tokenizer, batch_input)\n",
    "        \n",
    "        outputs = model.generate(**encode_inputs, max_new_tokens=1, pad_token_id=tokenizer.pad_token_id)\n",
    "        answers.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "    answers = [answer[-1] for answer in answers]\n",
    "    return answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c854fe3-f599-4cd7-bd25-55522206b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'llama'\n",
    "model_dir = 'models/Llama-2-7b-hf'\n",
    "param_size = '7'\n",
    "data_dir = 'benchmarks/MMLU/'\n",
    "ntrain = 5\n",
    "#model, tokenizer = load(model_dir, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4deb19-2fd8-4744-8f11-7e7d14be9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir, use_fast=False, padding_side=\"left\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e9f0b06-e695-4214-a317-62df28a02a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c34f2a7cb44aef8012b2513d13f236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(model_dir, attention_dropout=.1,  low_cpu_mem_usage = True, torch_dtype=torch.float16)\n",
    "model.to('cuda')\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1\n",
    "model.train() # turns on attention dropout, alternatively, we could set training = True for all the attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55e66220-53b4-4b2d-a9a7-e4288362231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(model.training)\n",
    "model.train()\n",
    "print(model.training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d77410ab-827f-4f43-a032-0387a53e26df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(1,1,10, dtype = torch.float32, device = 'cuda')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43bf003-c523-4026-a3cf-d5c66270b4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x/2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d8accef-c0f2-4ff1-9dda-dd470f041944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_confidence_agg(values, all_ids, model):\n",
    "    return torch.min(values,dim = -1)\n",
    "def max_confidence_agg(values, all_ids, model):\n",
    "    return torch.max(values,dim = -1)\n",
    "def avg_confidence_agg(values, all_ids, model):\n",
    "    return torch.mean(values,dim = -1)\n",
    "def attention_weighted_agg(values, all_ids, model):\n",
    "    #based on Guan et. al. 2023 Shifting Attention to Relevance\n",
    "    with torch.no_grad():\n",
    "        model_inputs = model.prepare_inputs_for_generation(all_ids, )\n",
    "        outputs = model(\n",
    "            **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=True,\n",
    "                output_hidden_states=False,\n",
    "        )\n",
    "    attn_weights = outputs['attentions'][0][-1,-1,-1,:]\n",
    "    return torch.dot(values,attn_weights)\n",
    "\n",
    "def logit_confidence(next_tokens_scores, next_token, all_ids, model):\n",
    "    return torch.max(next_tokens_scores,dim = -1)\n",
    "    \n",
    "def softmax_confidence(next_tokens_scores, next_token, all_ids, model):\n",
    "    return torch.max(torch.nn.functional.softmax(next_tokens_scores[-1,-1,:], dim = -1))\n",
    "    \n",
    "def entropy_confidence(next_tokens_scores, next_token, all_ids, model):\n",
    "    return Categorical(probs=torch.nn.functional.softmax(next_tokens_scores[-1,-1,:], dim = -1)).entropy()\n",
    "\n",
    "def ensemble_entropy_confidence(next_tokens_scores, next_token, all_ids, model, n_ensemble=5):\n",
    "    model.train()\n",
    "    estimate_vector = torch.zeros(next_tokens_scores.shape[-1], dtype = torch.float32, device = model.device)\n",
    "    for i in range(n_ensemble):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_inputs = model.prepare_inputs_for_generation(all_ids, )\n",
    "            outputs = model(\n",
    "                **model_inputs,\n",
    "                    return_dict=True,\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False,\n",
    "            )\n",
    "        next_tokens_scores = logits_processor(outputs['logits'][:,-1,:], outputs['logits'])\n",
    "        next_token = torch.argmax(next_tokens_scores[:,-1,:])\n",
    "        estimate_vector[next_token.item()] += 1\n",
    "        \n",
    "    estimate_vector = estimate_vector/n_ensemble # probabilities should sum to 1\n",
    "    model.eval()\n",
    "    return Categorical(probs=estimate_vector).entropy() \n",
    "\n",
    "def MMLU_self_reflection_confidence_promptv1(all_ids, n_prompt_tokens, model, tokenizer, prompt):\n",
    "    proposed_answer = tokenizer.decode(all_ids[-1*(all_ids.shape[-1] - n_prompt_tokens):])\n",
    "    prompt = prompt[:-7] # remove the \"Answer:\" at the end of the prompt\n",
    "    prompt = prompt + f'''Proposed Answer: {proposed_answer}\n",
    "Is the proposed answer:\n",
    "(A) True\n",
    "(B) False\n",
    "The proposed answer is: '''\n",
    "    encode_inputs = prepare_input(tokenizer, batch_input)\n",
    "    outputs = model.generate(**encode_inputs, max_new_tokens=1, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "    return 1 if outputs[-1,-1].item() == tokenizer.encode('A')[-1] else 0 #return 1 if model outputs 'A' else 0\n",
    "\n",
    "\n",
    "confidence_aggregation_funcs = [min_confidence_agg,max_confidence_agg,avg_confidence_agg,attention_weighted_agg]\n",
    "token_confidence_funcs = [logit_confidence,softmax_confidence,entropy_confidence, ensemble_entropy_confidence]\n",
    "sequence_confidence_funcs = [MMLU_self_reflection_confidence_promptv1, ]\n",
    "\n",
    "\n",
    "def prepare_prompt(tokenizer, prompt):\n",
    "    input_tokens = tokenizer.encode_plus(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_tokens = {\n",
    "            k:input_tokens[k] for k in input_tokens if k in [\"input_ids\", \"attention_mask\"]\n",
    "    }\n",
    "    for t in input_tokens:\n",
    "        if torch.is_tensor(input_tokens[t]):\n",
    "            input_tokens[t] = input_tokens[t].to('cuda')\n",
    "    return input_tokens\n",
    "\n",
    "def generate_with_confidence_funcs(model, tokenizer, prompt, max_new_tokens, token_confidence_funcs, confidence_aggregation_funcs, sequence_confidence_funcs ):\n",
    "    all_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device) \n",
    "    n_prompt_tokens = all_ids.shape[-1]\n",
    "    logits_processor = LogitsProcessorList()\n",
    "    stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=n_prompt_tokens+max_new_tokens)])\n",
    "    pad_token_id = model.generation_config.pad_token_id\n",
    "    eos_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "    all_token_confidences = {}\n",
    "    sequence_confidences = {}\n",
    "\n",
    "    while True:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_inputs = model.prepare_inputs_for_generation(all_ids, )\n",
    "        \n",
    "\n",
    "            outputs = model(\n",
    "                **model_inputs,\n",
    "                    return_dict=True,\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False,\n",
    "            )\n",
    "        \n",
    "        next_tokens_scores = logits_processor(outputs['logits'][:,-1,:], outputs['logits'])\n",
    "        next_token = torch.argmax(next_tokens_scores[:,-1,:])\n",
    "\n",
    "        token_confidences = { id: val for id, val in [func(next_tokens_scores, next_token, all_ids, model) for func in token_confidence_funcs]}\n",
    "\n",
    "        for id, val in token_confidences.items():\n",
    "            all_token_confidences[id] = all_token_confidences.get(id,[])\n",
    "            all_token_confidences[id].append(val)\n",
    "        \n",
    "        all_ids = torch.cat([all_ids,next_token.unsqueeze(0)],axis = -1)\n",
    "\n",
    "        if stopping_criteria(all_ids, next_tokens_scores):\n",
    "            break\n",
    "    answer = tokenizer.decode(all_ids)[-1]\n",
    "\n",
    "\n",
    "    for func in confidence_aggregation_funcs:\n",
    "        for token_confidence_id , values in all_token_confidences.items():\n",
    "            agg_id , val = func(values, all_ids, model)\n",
    "            sequence_confidences[agg_id+'|'+token_confidence_id] = val\n",
    "\n",
    "    for func in sequence_confidence_funcs:\n",
    "        seq_confidence_id , val = func(next_tokens_scores, next_token,all_ids, model, tokenizer, prompt)\n",
    "        sequence_confidences[seq_confidence_id] = val       \n",
    "\n",
    "    return [answer], sequence_confidences\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44c2e181-f2c1-4907-85ad-0b6590e0299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'ARLAKJDFLJKASDJKL:DASJ:KLDLjk;'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "593e962b-2012-475a-9b78-e398ef2e6857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_inputs['input_ids'][-1,-1].item() == tokenizer.encode('B')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db5fe716-e0e6-4e18-817b-d085df2d1b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encode_inputs['input_ids'][-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cdae5fb7-db04-4202-8278-019eb4718dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('B')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38db718c-6514-497d-8059-214198d34512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   319,  2241, 22311, 29967,  4037, 29931, 29967, 29968,  3289,\n",
       "         29928, 29967, 29968, 29931, 29901, 29928,  3289, 29967, 29901, 29968,\n",
       "         10249, 29931, 25467, 29936,   350, 29936]], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "201627d7-6aca-485c-816e-c9bc4075d96c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'next_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnext_token\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'next_token' is not defined"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**encode_inputs, max_new_tokens=1, pad_token_id=tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d655eb0-e85c-4172-9b63-14d1e029d5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   319,  2241, 22311, 29967,  4037, 29931, 29967, 29968,  3289,\n",
       "         29928, 29967, 29968, 29931, 29901, 29928,  3289, 29967, 29901, 29968,\n",
       "         10249, 29931, 25467, 29936,   350, 29936]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'ARLAKJDFLJKASDJKL:DASJ:KLDLjk; B'\n",
    "encode_inputs = prepare_prompt(tokenizer,prompt)\n",
    "outputs = model.generate(**encode_inputs, max_new_tokens=1, pad_token_id=tokenizer.pad_token_id)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67f23bb2-528e-49aa-a52e-b4ef56b27884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "';'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encode_inputs['input_ids'][-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96dd8a48-eb7c-4ac4-858d-9ccdf408b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'THIS IS MY PROMPT MY PROMPT IS GOOD'\n",
    "all_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda') \n",
    "n_tokens = all_ids.shape[-1]\n",
    "\n",
    "logits_processor = LogitsProcessorList()\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=n_tokens)])\n",
    "pad_token_id = model.generation_config.pad_token_id\n",
    "eos_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_inputs = model.prepare_inputs_for_generation(all_ids, )\n",
    "\n",
    "    outputs = model(\n",
    "        **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "    )\n",
    "\n",
    "next_tokens_scores = logits_processor(outputs['logits'][:,-1,:], outputs['logits'])\n",
    "\n",
    "next_token = torch.argmax(next_tokens_scores[:,-1,:])\n",
    "generated_token = tokenizer.decode(next_token)\n",
    "mean_attn_weight = outputs['attentions'][0].squeeze()[:,-1,:].mean(axis = 0)\n",
    "entropy = Categorical(probs=torch.nn.functional.softmax(next_tokens_scores[-1,-1,:], dim = -1)).entropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "861ddd5c-09a5-420d-82dd-568e8e82e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = torch.argmax(next_tokens_scores[:,-1,:],axis = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8345195-19c5-409c-ab70-2e46fbb601d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  3446,  3235,  8519, 19519,   349,  3491,  7982, 19519,   349,\n",
       "          3491,  7982,  8519, 21947, 13668,    13]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([all_ids,next_token.unsqueeze(0)],axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7143bf55-f337-46b9-b0c0-31ed05ba39e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_tokens_scores.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7e0fcc6f-b20a-4f01-89ff-95a902175c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TagsIS IS A FERI FOR WROMPT IS AING M']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(torch.argmax(next_tokens_scores,axis = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56327dbc-2f0d-49db-a494-203989fd317e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1174, 0.0569, 0.0568, 0.0371, 0.0476, 0.0797, 0.0546, 0.0548, 0.0486,\n",
       "        0.0847, 0.0648, 0.0690, 0.0579, 0.0759, 0.0941], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['attentions'][0].squeeze()[:,-1,:].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef4cd235-ccfa-42a5-97fe-1bee269c4050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'][:,-1,:]\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d2d9fd-bb53-4a6b-88ce-8e0736bc46c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing abstract_algebra ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:04<00:16,  4.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     label \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39miloc[i, test_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     21\u001b[0m     records\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m:prompt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m:label})\n\u001b[0;32m---> 23\u001b[0m pred_answers \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m gold_answers \u001b[38;5;241m=\u001b[39m [record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records]\n\u001b[1;32m     25\u001b[0m run_results[task] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_answers\u001b[39m\u001b[38;5;124m'\u001b[39m:pred_answers, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgold_answers\u001b[39m\u001b[38;5;124m'\u001b[39m:gold_answers}\n",
      "Cell \u001b[0;32mIn[12], line 181\u001b[0m, in \u001b[0;36mbatch_infer\u001b[0;34m(model, tokenizer, prompts)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_input \u001b[38;5;129;01min\u001b[39;00m tqdm(batch_split(prompts, batch_size)):\n\u001b[1;32m    180\u001b[0m     encode_inputs \u001b[38;5;241m=\u001b[39m prepare_input(tokenizer, batch_input)\n\u001b[0;32m--> 181\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencode_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     answers\u001b[38;5;241m.\u001b[39mextend(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    183\u001b[0m answers \u001b[38;5;241m=\u001b[39m [answer[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m answer \u001b[38;5;129;01min\u001b[39;00m answers]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1718\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1702\u001b[0m         input_ids,\n\u001b[1;32m   1703\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1715\u001b[0m     )\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2636\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2631\u001b[0m     unfinished_sequences \u001b[38;5;241m=\u001b[39m unfinished_sequences\u001b[38;5;241m.\u001b[39mmul(\n\u001b[1;32m   2632\u001b[0m         next_tokens\u001b[38;5;241m.\u001b[39mtile(eos_token_id_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mne(eos_token_id_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mprod(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2633\u001b[0m     )\n\u001b[1;32m   2635\u001b[0m     \u001b[38;5;66;03m# stop when each sentence is finished\u001b[39;00m\n\u001b[0;32m-> 2636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unfinished_sequences\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2637\u001b[0m         this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2639\u001b[0m \u001b[38;5;66;03m# stop if we exceed the maximum length\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_results = {}\n",
    "output_filename = 'run_results_%s_%sb.json' % (model_type, param_size)\n",
    "\n",
    "start_time = time.time()\n",
    "for task in TASKS:\n",
    "    print('Testing %s ...' % task)\n",
    "    records = []\n",
    "    dev_df = pd.read_csv(os.path.join(data_dir, \"dev\", task + \"_dev.csv\"), header=None)[:ntrain]\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, \"test\", task + \"_test.csv\"), header=None)\n",
    "    for i in range(test_df.shape[0]):\n",
    "        # get prompt and make sure it fits\n",
    "        k = ntrain\n",
    "        prompt_end = format_example(test_df, i, include_answer=False)\n",
    "        train_prompt = gen_prompt(dev_df, task, k)\n",
    "        prompt = train_prompt + prompt_end\n",
    "        while len(tokenizer.tokenize(prompt)) + 1> 2048: # bos token\n",
    "            prompt_split = prompt.split(\"\\n\\n\")\n",
    "            prompt_split.pop(1)\n",
    "            prompt = '\\n\\n'.join(prompt_split)\n",
    "        label = test_df.iloc[i, test_df.shape[1]-1]\n",
    "        records.append({'prompt':prompt, 'answer':label})\n",
    "\n",
    "    pred_answers = batch_infer(model, tokenizer, [record['prompt'] for record in records])\n",
    "    gold_answers = [record['answer'] for record in records]\n",
    "    run_results[task] = {'pred_answers':pred_answers, 'gold_answers':gold_answers}\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(run_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "compute_metric(output_filename)\n",
    "end_time = time.time()\n",
    "print(\"total run time %.2f\" % (end_time - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed97215b-c54e-48db-b3ae-66dd3f233326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_metric(output_filename):\n",
    "    with open(output_filename, 'r') as f:\n",
    "        run_results = json.load(f)\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    for task in run_results:\n",
    "        acc = 0\n",
    "        pred_answers = run_results[task]['pred_answers']\n",
    "        gold_answers = run_results[task]['gold_answers']\n",
    "        for pred, gold in zip(pred_answers, gold_answers):\n",
    "            if pred == gold: acc += 1\n",
    "        print(\"ACC-%s: %.4f\" % (task, acc/len(gold_answers)))\n",
    "        total_acc += acc\n",
    "        total_num += len(gold_answers)\n",
    "    print(\"ACC-all: %.4f\" % (total_acc/total_num))\n",
    "\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "def format_example(df, idx, include_answer=True):\n",
    "    prompt = df.iloc[idx, 0]\n",
    "    k = df.shape[1] - 2\n",
    "    for j in range(k):\n",
    "        prompt += \"\\n{}. {}\".format(choices[j], df.iloc[idx, j+1])\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    if include_answer:\n",
    "        prompt += \" {}\\n\\n\".format(df.iloc[idx, k + 1])\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(train_df, subject, k=-1):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(format_subject(subject))\n",
    "    if k == -1:\n",
    "        k = train_df.shape[0]\n",
    "    for i in range(k):\n",
    "        prompt += format_example(train_df, i)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# def custom_stopping_criteria(input_ids, score, **kwargs):\n",
    "#     stop_ids = [29871, 13, 13] # \\n\\n \n",
    "#     return input_ids[-len(stop_ids)]\n",
    "\n",
    "def prepare_input(tokenizer, prompts):\n",
    "    input_tokens = tokenizer.batch_encode_plus(prompts, return_tensors=\"pt\", padding=True)\n",
    "    input_tokens = {k:input_tokens[k] for k in input_tokens if k in [\"input_ids\", \"attention_mask\"]}\n",
    "    for t in input_tokens:\n",
    "        if torch.is_tensor(input_tokens[t]):\n",
    "            input_tokens[t] = input_tokens[t].to('cuda')\n",
    "\n",
    "    return input_tokens\n",
    "\n",
    "def load(ckpt_dir, model_type):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "\n",
    "    if model_type == 'llama':\n",
    "        # we use tensor parallel for loading llama\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(ckpt_dir, use_fast=False, padding_side=\"left\")\n",
    "        \n",
    "        model = LlamaForCausalLM.from_pretrained(ckpt_dir, low_cpu_mem_usage = True, torch_dtype=torch.float16)\n",
    "        model = tp.tensor_parallel(model, [i for i in range(n_gpus)])\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'flan':\n",
    "        # we use tensor parallel for loading llama\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, use_fast=False, padding_side=\"left\")\n",
    "        \n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(ckpt_dir, low_cpu_mem_usage = True, torch_dtype=torch.float16)\n",
    "        model = tp.tensor_parallel(model, [i for i in range(n_gpus)])\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'falcon':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir, device_map = 'balanced_low_0', torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'moss':\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        config = AutoConfig.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "        model.tie_weights()\n",
    "        model = load_checkpoint_and_dispatch(model, model_path, device_map=\"auto\", no_split_module_classes=[\"MossBlock\"], dtype=torch.float16)\n",
    "        \n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "    elif model_type == 'guanaco':\n",
    "\n",
    "\n",
    "\n",
    "        model_name = \"llama-65b\"\n",
    "        adapters_name = 'guanaco-65b'\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            load_in_4bit=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            max_memory= {i: '24000MB' for i in range(torch.cuda.device_count())},\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type='nf4'\n",
    "            ),\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, adapters_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    elif model_type == 'vicuna':\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir, device_map = 'balanced_low_0', revision=\"main\", trust_remote_code=False)\n",
    "\n",
    "\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "        \n",
    "    elif model_type == 'starcoder':\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir,device_map = 'balanced_low_0', trust_remote_code=True)\n",
    "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "        tokenizer.bos_token_id = 1\n",
    "        \n",
    "\n",
    "    else:\n",
    "        # mpt-30b's tokenizer only has the fast version\n",
    "        use_fast = \"mosaicml/mpt-30b\" in ckpt_dir\n",
    "        # however, tensor parallel for running falcon will occur bugs\n",
    "        tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, use_fast = use_fast, padding_side=\"left\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_dir, device_map = 'balanced_low_0', torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            if tokenizer.eos_token_id is not None:\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            else:\n",
    "                tokenizer.pad_token_id = 0\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def batch_split(prompts, batch_num):\n",
    "    batch_prompts = []\n",
    "    mini_batch = []\n",
    "    for prompt in prompts:\n",
    "        mini_batch.append(prompt)\n",
    "        if len(mini_batch) == batch_num:\n",
    "            batch_prompts.append(mini_batch)\n",
    "            mini_batch = []\n",
    "    if len(mini_batch) != 0:\n",
    "        batch_prompts.append(mini_batch)\n",
    "    return batch_prompts\n",
    "\n",
    "def batch_infer(model, tokenizer, prompts):\n",
    "    batch_size = 1\n",
    "    answers = []\n",
    "    for batch_input in tqdm(batch_split(prompts, batch_size)):\n",
    "        encode_inputs = prepare_input(tokenizer, batch_input)\n",
    "        outputs = model.generate(**encode_inputs, max_new_tokens=1, pad_token_id=tokenizer.pad_token_id)\n",
    "        answers.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "    answers = [answer[-1] for answer in answers]\n",
    "    return answers\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
